This sidetrack manifested primarily as a deep dive into the TensorFlow Quantum library, sort of an extension from the well known TensorFlow library for training neural networks. The progress made in the field since that 2018 presentation has been considerable, with nearly every nook of machine learning now finding some theoretic claim staked within QML. Interestingly, this progression of QML has sort of followed in a similar trajectory as the path first trodden decades ago with classical machine learning, as the early branches to establish beachheads were applications like clustering algorithms, kernel methods, principle components, things like that. A common similarity between these methods were the use of quantum algorithms that could speed up linear algebra operations, such as the calculation of eigenvalues, using quantum algorithms like HHL or QSVT. Other applications could make use of quantum optimization algorithms like QAOA. Of course while these speedups were demonstrated in theory, their potential in practice has been somewhat constrained under the reality of current generations of gate based quantum computing hardware, which are still in the NISQ (noisy intermediate scale quantum) era lacking sufficient noise tolerance to reach scale for fault tolerance enabling error correction.